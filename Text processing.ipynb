{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2455b14",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "### Q1\n",
    "\n",
    "1. Modify the code I wrote in lecture 8 with what you have learnt in lecture 9 and correctly tokenize the text both on the word and sentence level, and by removing the stopwords. Rewrite the `getSummary` function and all the other functions that it depends by maing these corrections.\n",
    "\n",
    "2. Rewrite the code I wrote for `getKeywords` function making the same corrections.\n",
    "\n",
    "3. Test your code from parts 1 and 2 on random articles from the Guardian.\n",
    "\n",
    "4. Rewrite the `getSubjectGuardian` function for another newspaper in English, and test your code from part 1 and 2 on random articles from this new newspaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952b4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from snowballstemmer import TurkishStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from collections import Counter\n",
    "from xmltodict import parse\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e323e05",
   "metadata": {},
   "source": [
    "## Modifying getSummary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4beda5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectGuardian(subject):\n",
    "    with requests.get(f'https://www.theguardian.com/{subject}/rss') as link:\n",
    "        raw = parse(link.text)\n",
    "    return raw['rss']['channel']['item']\n",
    "\n",
    "def getText(url):\n",
    "    with requests.get(url) as link:\n",
    "        raw = BeautifulSoup(link.content,'html.parser')\n",
    "    return ' '.join([x.text for x in raw.find_all('p')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc73b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a11b970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantext(text) :\n",
    "    OMATS = text\n",
    "    omats = {'sentences': sent_tokenize(OMATS)}\n",
    "    omats.update({'cleanedSentences': [re.sub(r'[^\\p{Letter}\\s]','',sentence.lower()) for sentence in omats['sentences']]})\n",
    "    \n",
    "    return omats[\"cleanedSentences\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9d9ab5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrix(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dab93bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummary(text,k):\n",
    "    sentences = cleantext(text)\n",
    "    matrix = getMatrix(sentences)\n",
    "    projection = PCA(n_components=1)\n",
    "    weights = projection.fit_transform(matrix.toarray())\n",
    "    res = list(zip(weights.transpose()[0],range(500),sentences))\n",
    "    tmp = sorted(res,key=lambda x: x[0],reverse=True)[:k]\n",
    "    return sorted(tmp, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0167f",
   "metadata": {},
   "source": [
    "### Testing the getSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "11b21219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.8401787753513408,\n",
       "  5,\n",
       "  'the american has partnered with his fellow dodgers owner mark walter the swiss billionaire hansjörg wyss the british property developer jonathan goldstein and the us investment firm clearlake capital and they expect to be granted exclusivity'),\n",
       " (6.329292920675048,\n",
       "  7,\n",
       "  'there was confusion when ratcliffe the owner of the british petrochemicals company ineos revealed on friday that he had made his move to buy the european champions a fortnight after the deadline for final bids britains richest man offered to pay bn for chelsea and included a pledge to invest bn in the club over the next  years'),\n",
       " (4.279477888440978,\n",
       "  15,\n",
       "  'abramovich who has also vowed to write off bn in loans he has given to chelsea since  pledged to donate the net proceeds of the sale to all victims of the war in ukraine before he was sanctioned by the british government last month'),\n",
       " (2.8620350015444895,\n",
       "  24,\n",
       "  'bn is committed to the charitable trust to support victims of the war with bn committed to investment directly into the club over the next  years'),\n",
       " (2.57444256756531,\n",
       "  25,\n",
       "  'this is a british bid for a british club ineos which has taken over the former team sky cycling team and has a stake in the mercedes formula one team said there would be investment in stamford bridge the mens and womens first teams and the academy')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economy = getSubjectGuardian('economy')\n",
    "n = np.random.randint(5)\n",
    "text = getText(economy[n]['link'])\n",
    "getSummary(text,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccccec2",
   "metadata": {},
   "source": [
    "## New Newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fd2970dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectNytimes(subject):\n",
    "    with requests.get(f'https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml') as link:\n",
    "        raw = parse(link.text)\n",
    "    return raw['rss']['channel']['item']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de420bd",
   "metadata": {},
   "source": [
    "### Testing new newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4b3739f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.nytimes.com/2022/04/29/us/politics/ukraine-rape-war-crimes.html'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economy = getSubjectNytimes('economy')\n",
    "n = np.random.randint(4)\n",
    "economy[n][\"link\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f14225",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = getText(economy[1]['link'])\n",
    "\n",
    "getSummary(text,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaffa83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c148a291",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "Write a function that returns all named entities (proper names, country names, corporation names only) from a URL. Function should take the URL as the input and must return the list of named entities from that URL. Test your code on random articles from the Guardian. Don't use the NLTK's NER that I demonstrated during the lecture. Use the SpaCY's NER function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "39ef7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectGuardian(subject):\n",
    "    with requests.get(f'https://www.theguardian.com/{subject}/rss') as link:\n",
    "        raw = parse(link.text)\n",
    "    return raw['rss']['channel']['item']\n",
    "\n",
    "\n",
    "def getText(url):\n",
    "    with requests.get(url) as link:\n",
    "        raw = BeautifulSoup(link.content,'html.parser')\n",
    "    return ' '.join([x.text for x in raw.find_all('p')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "12046b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60f95c",
   "metadata": {},
   "source": [
    "### Creating the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1ba4e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities (url) :\n",
    "    res = NER(getText(url))\n",
    "    words =[]\n",
    "    for word in res.ents:\n",
    "        if spacy.explain(word.label_) == (\"Countries, cities, states\") :\n",
    "            words.append((word.text,spacy.explain(word.label_)))\n",
    "        if spacy.explain(word.label_) ==(\"Companies, agencies, institutions, etc.\"):\n",
    "            words.append((word.text,spacy.explain(word.label_)))\n",
    "        if spacy.explain(word.label_) == (\"People, including fictional\"):\n",
    "            words.append((word.text,spacy.explain(word.label_)))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81832e9",
   "metadata": {},
   "source": [
    "### Testing the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2ddc2b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BP', 'Countries, cities, states'),\n",
       " ('Shell', 'Companies, agencies, institutions, etc.'),\n",
       " ('BP', 'Companies, agencies, institutions, etc.'),\n",
       " ('Kwasi Kwarteng', 'People, including fictional'),\n",
       " ('UK', 'Countries, cities, states'),\n",
       " ('Times', 'Companies, agencies, institutions, etc.'),\n",
       " ('Rishi Sunak', 'People, including fictional'),\n",
       " ('Mumsnet', 'Companies, agencies, institutions, etc.'),\n",
       " ('Britain', 'Countries, cities, states'),\n",
       " ('UK', 'Countries, cities, states'),\n",
       " ('OEUK', 'Companies, agencies, institutions, etc.'),\n",
       " ('UK', 'Countries, cities, states'),\n",
       " ('OEUK', 'Companies, agencies, institutions, etc.'),\n",
       " ('UK', 'Countries, cities, states'),\n",
       " ('Shell', 'Companies, agencies, institutions, etc.'),\n",
       " ('UK', 'Countries, cities, states'),\n",
       " ('BP', 'Countries, cities, states'),\n",
       " ('UK', 'Countries, cities, states'),\n",
       " ('BP', 'Companies, agencies, institutions, etc.'),\n",
       " ('Shell', 'Companies, agencies, institutions, etc.'),\n",
       " ('Harbour Energy', 'Companies, agencies, institutions, etc.'),\n",
       " ('UK', 'Countries, cities, states'),\n",
       " ('£', 'Companies, agencies, institutions, etc.'),\n",
       " ('UK', 'Countries, cities, states'),\n",
       " ('the Office for Budget Responsibility',\n",
       "  'Companies, agencies, institutions, etc.'),\n",
       " ('The Office for National Statistics',\n",
       "  'Companies, agencies, institutions, etc.'),\n",
       " ('Bulb', 'People, including fictional')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economy = getSubjectGuardian('economy')\n",
    "n = np.random.randint(4)\n",
    "named_entities(economy[n]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2569bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fbc45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82d80aaa",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "1. Write a function that returns the most positive and the most negative sentences from a text. The function must take the text as the input and must return a 2-tuple: the first element as the most positive and the second as the most negative sentence with their polarity scores.\n",
    "\n",
    "2. Test your function on random articles from the Guardian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "095fd4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd8aed",
   "metadata": {},
   "source": [
    "### Creating the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ed5bcede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_p_and_n_Scores (text):\n",
    "\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    m_negative_score = 0\n",
    "    m_positive_score = 0\n",
    "\n",
    "    \n",
    "    for x in sentences:\n",
    "        negative_score = analyzer.polarity_scores(x)[\"neg\"]\n",
    "        positive_score = analyzer.polarity_scores(x)[\"pos\"]\n",
    "        if negative_score>= m_negative_score:\n",
    "            m_negative_score = negative_score\n",
    "            m_negative_sentence = x\n",
    "            scores_of_neg = analyzer.polarity_scores(x)\n",
    "        if positive_score >= m_positive_score:\n",
    "            m_positive_score = positive_score\n",
    "            m_positive_sentence = x\n",
    "            scores_of_pos = analyzer.polarity_scores(x)\n",
    "        \n",
    "    \n",
    "        \n",
    "    return (m_positive_sentence,scores_of_pos,m_negative_sentence,scores_of_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caa0f3",
   "metadata": {},
   "source": [
    "### Testing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c8255d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "economy = getSubjectGuardian('economy')\n",
    "n = np.random.randint(4)\n",
    "text = getText(economy[n]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2b92cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A similar dynamic is happening now with lorries.',\n",
       " {'neg': 0.0, 'neu': 0.698, 'pos': 0.302, 'compound': 0.3818},\n",
       " 'The problems are range and cost.',\n",
       " {'neg': 0.351, 'neu': 0.649, 'pos': 0.0, 'compound': -0.4019})"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_p_and_n_Scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab61e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f35945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
